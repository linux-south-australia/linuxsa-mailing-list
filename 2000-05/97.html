<HTML>
<!-- EmailData="Start" -->
<!-- Version="1.1" -->
<!-- Subject="Thank you for repling (Re: file grabber for web pages ??)" -->
<!-- FromName="Jason Nunn" -->
<!-- FromEmail="jsno@downunder.net.au" -->
<!-- ToName="Mike Andrew" -->
<!-- ToEmail="mikero@norfolk.nf" -->
<!-- Date="Wed, 3 May 2000 19:26:16 +0930 (CST)" -->
<!-- Id="Pine.LNX.4.04.10005031907560.379-100000@berkeley.localnet" -->
<!-- Reference="00050301550305.20568@mikero.nf" -->
<!-- X-Face="" -->
<!-- X-URL="" -->
<!-- EmailData="End" -->
<HEAD>
<TITLE>LinuxSA Mailing List: Thank you for repling (Re: file grabber for web pages ??)</TITLE>
</HEAD>
<BODY BGCOLOR=#FFFFFF><H1>LinuxSA Mailing list archives</H1>
<!-- IndexControl1="Start" -->
Index:
[<A HREF="thread.html">thread</A>]
[<A HREF="date.html">date</A>]
[<A HREF="subject.html">subject</A>]
[<A HREF="author.html">author</A>]
<HR>
<!-- IndexControl1="End" -->
<!-- Header="Start" -->
<PRE>
  From: Jason Nunn &lt;<I><A HREF="mailto:jsno@downunder.net.au">jsno@downunder.net.au</A></I>&gt;
  To  : Mike Andrew &lt;<I><A HREF="mailto:mikero@norfolk.nf">mikero@norfolk.nf</A></I>&gt;
  Date: Wed, 3 May 2000 19:26:16 +0930 (CST)
</PRE>
<H1>Thank you for repling (Re: file grabber for web pages ??)</H1>
<!-- Header="End" -->
<!-- Body="Start" -->
<PRE>
hello mike and everybody who has replied to me

Mike Andrew &lt;<A HREF="mailto:mikero@norfolk.nf">mikero@norfolk.nf</A>&gt; writes:

&gt; &gt; I'm after a utility that can read a web page, and download any file links
&gt; &gt; on that page.
&gt; 
&gt; wget -mnv &lt;url of site&gt;

the consenus seems to be "wget". i've downloaded and had a look at this
tool, but it's far too clumsy for what i want. it seems to be geared for
sucking down entire web sites!. you point to a page, and it goes to pieces
downloading everything it can see. i just want to suck down a few 100 or
so *.zip files from a selected part of 1 page.

this is what i what-

- point to a page, and tell program to download *.zip files, or *.gz
files, or */cgi/binaries/linux*.zip files or whatever you specify.

for example:

PROGRAM -U <A HREF="http://www.linuxsa.org.au/page.html">http://www.linuxsa.org.au/page.html</A> -P *.zip


- better still, be able to direct html script (or a part of a page) to the
utility, and for that utility to parse the html, read the URL's of it, and
download the specified data.

PROGRAM -S www.linuxsa.org.au:80 -P *.zip &lt; page.html

in this scenerio, i would save a page, delete all the crap i don't want,
and then direct it to the program.


I might end up writing my own..

thanks anyway... and it looks like i posted at a bum-ticker of a time too
;). talk about subscribing at a wrong time ;). i should have sat for a bit
and read current threads before blurting out "how do i suck down web
sites" ;) ;). the course subtlety was totally unintentional ;)

see ya

--
Jason Nunn- Darwin, Northern Territory-  www.downunder.net.au/~jsno

-- 
LinuxSA WWW: <A HREF="http://www.linuxsa.org.au/">http://www.linuxsa.org.au/</A>  IRC: #linuxsa on irc.linux.org.au
To unsubscribe from the LinuxSA list:
  mail <A HREF="mailto:linuxsa-request@linuxsa.org.au">linuxsa-request@linuxsa.org.au</A> with "unsubscribe" as the subject

</PRE>
<!-- Body="End" -->
<!-- IndexControl2="Start" -->
<HR>
Index:
[<A HREF="thread.html">thread</A>]
[<A HREF="date.html">date</A>]
[<A HREF="subject.html">subject</A>]
[<A HREF="author.html">author</A>]
<!-- IndexControl2="End" -->
<HR><FONT SIZE=+1>Return to the <A HREF=/mailing-list/>LinuxSA Mailing List Information</A> Page</FONT></BODY>
</HTML>
